# From Beat to Flight

### Music-Driven Drone Choreography for Expressive Humanâ€“Drone Interaction

---

## Overview

**From Beat to Flight** explores how Music Information Retrieval (MIR) can dynamically program expressive drone movements for performance and humanâ€“drone interaction (HDI).

Instead of manually scripting aerial choreography, this project investigates how musical structure â€” tempo, pitch, rhythm, timbre, and harmonic features â€” can be computationally parsed and translated into meaningful drone behaviors.

The long-term vision is to enable drones to function as:

* ğŸµ Expressive aerial dancers
* ğŸ¤ Human dance partners
* ğŸŒŠ Swarm-based embodied music environments

This project contributes a **musicâ€“droneâ€“movement design space**, a technical mapping framework, and a series of empirical studies examining how humans perceive, co-create, and move with music-responsive drones.

---

## Research Goals

1. Define a **Musicâ€“Droneâ€“Movement Design Space**
2. Develop a real-time MIR-to-drone mapping pipeline
3. Evaluate drone expressivity and perceived musicality
4. Investigate drones as embodied co-dancers
5. Explore swarm behavior as immersive music-responsive environments

---

# Project Phases

## Phase 1 â€” Musicâ€“Droneâ€“Movement Design Space

* [ ] Define MIR feature taxonomy
* [ ] Define drone motion primitives
* [ ] Formalize mapping strategies (1:1, hierarchical, swarm-based)
* [ ] Implement real-time MIR parsing pipeline
* [ ] Conduct video-based perception study (expressiveness & musicality)
* [ ] Publish design space contribution

---

## Phase 2 â€” Drone as Dance Partner

* [ ] Implement single-drone monophonic mapping system
* [ ] Compare fixed choreography vs beat-reactive vs semantic MIR mapping
* [ ] Conduct humanâ€“drone co-dance user study
* [ ] Measure perceived agency
* [ ] Measure flow
* [ ] Measure co-creativity
* [ ] Measure musical alignment
* [ ] Measure safety perception
* [ ] Publish HDI study

---

## Phase 3 â€” Swarm as Embodied Music Field

* [ ] Extend mapping to multi-track music â†’ multi-drone swarm
* [ ] Implement 1-track-to-1-drone mapping paradigm
* [ ] Design swarm behaviors (density, proximity, formation morphing)
* [ ] Prototype "moshpit" interaction scenario
* [ ] Conduct embodied immersion study
* [ ] Measure emotional synchrony
* [ ] Measure spatial awareness
* [ ] Measure immersion
* [ ] Measure risk comfort
* [ ] Publish swarm interaction study

---

# Musicâ€“Droneâ€“Movement Mapping Space

The core technical contribution is a structured mapping between MIR features and drone motion parameters.

| MIR Feature        | Drone Parameter                | Expressive Rationale                   |
| ------------------ | ------------------------------ | -------------------------------------- |
| Tempo (BPM)        | Velocity                       | Faster tempo â†’ increased motion energy |
| Beat Onset         | Acceleration pulses            | Perceived rhythmic synchronization     |
| Pitch Height       | Altitude                       | High pitch â†’ elevated position         |
| Harmonic Tension   | Proximity to human             | Dissonance â†’ spatial tension           |
| Spectral Energy    | Swarm density                  | High energy â†’ tighter formation        |
| Timbre Brightness  | LED color intensity            | Brighter timbre â†’ higher luminance     |
| Polyphony          | Number of active drones        | Each track â†’ one expressive agent      |
| Amplitude Envelope | Vertical oscillation amplitude | Dynamic shaping of motion              |

---

## Mapping Paradigms

### 1:1 Mapping

Monophonic channel â†’ Single drone

### Hierarchical Mapping

Bass â†’ grounding drone
Melody â†’ lead expressive drone
Percussion â†’ high-frequency motion drones

### Swarm Mapping

Multi-track music â†’ distributed drone ensemble

---

# Technical Architecture (Planned)

* Audio Input (WAV / Live Stream)
* MIR Feature Extraction (onset detection, tempo estimation, spectral analysis)
* Mapping Engine
* Motion Primitive Generator
* Drone Control Interface (SDK / ROS / Swarm API)

---

# Research Questions

* Can MIR-driven drone motion be perceived as expressive and musical?
* Does semantic mapping improve perceived intentionality?
* Can drones function as co-creative dance partners?
* How does swarm density influence embodied immersion?
* What defines a meaningful musicâ€“droneâ€“movement design space?

---

# Long-Term Vision

To establish **music-responsive aerial robotics** as a new medium for:

* Performance art
* Embodied interaction
* Humanâ€“swarm collaboration
* Creative AI scaffolding in HDI

---

# Repository Status

ğŸš§ Early-stage research project
ğŸ“¡ Active development
ğŸµ Experimental mapping engine in progress
